# -*- coding: utf-8 -*-
"""BestCluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19GosAPnj3Luw6MWdkKUH8tpzyDx2Hve2
"""

import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score

def compare_algorithms(X, max_clusters):
  results = []
  cluster_range = range(2, max_clusters + 1)

  # KMeans
  for n_clusters in cluster_range:
    kmeans = KMeans(n_clusters = n_clusters, random_state = 0, n_init = 'auto')
    clusters = kmeans.fit_predict(X)
    silhouette_avg = silhouette_score(X, clusters)
    results.append(('KMeans', n_clusters, silhouette_avg))

  # Agglomerative
  for n_clusters in cluster_range:
    agglomerative = AgglomerativeClustering(n_clusters = n_clusters)
    clusters = agglomerative.fit_predict(X)
    silhouette_avg = silhouette_score(X, clusters)
    results.append(('Agglomerative', n_clusters, silhouette_avg))

  # DBSCAN
  eps_values = np.arange(0.1, 0.9, 0.1)
  for eps in eps_values:
    dbscan = DBSCAN(eps = eps, min_samples = 5)
    clusters = dbscan.fit_predict(X)
    if (len(set(clusters)) > 1):
      silhouette_avg = silhouette_score(X, clusters)
      results.append(('DBSCAN', eps, silhouette_avg))

  return results

iris = datasets.load_iris()
scaler = StandardScaler()
scaled_data = scaler.fit_transform(iris.data)

results = compare_algorithms(scaled_data, 10)
df = pd.DataFrame(results, columns = ['Agrupador', 'Clusters', 'Score'])
df

max_score_index = df['Score'].idxmax()
print(df.loc[max_score_index])