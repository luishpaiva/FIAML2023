# -*- coding: utf-8 -*-
"""NLPRna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YBTUgRcefhY645jxaW3pS2zuGY_U7Fvq
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Embedding
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

from google.colab import files
files.upload()

spam = pd.read_csv("spam.csv")
spam.head()

spam.shape

labelencoder = LabelEncoder()
y = labelencoder.fit_transform(spam['Category'])
print(y)

mensagens = spam['Message'].values
X_train, X_test, y_train, y_test = train_test_split(mensagens, y, test_size = 0.3)

token = Tokenizer(num_words = 1000)
token.fit_on_texts(X_train)
X_train = token.texts_to_sequences(X_train)
X_test = token.texts_to_sequences(X_test)

print(X_train)

X_train = pad_sequences(X_train, padding = "post", maxlen = 500)
X_test = pad_sequences(X_test, padding = "post", maxlen = 500)

len(token.word_index)

X_train

token.word_index

modelo = Sequential()
modelo.add(Embedding(input_dim = len(token.word_index), output_dim = 50, input_length = 500))
modelo.add(Flatten())
modelo.add(Dense(units = 10, activation = 'relu'))
modelo.add(Dropout(0.1))
modelo.add(Dense(units = 1, activation = 'sigmoid'))

modelo.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ["accuracy"])

modelo.summary()

modelo.fit(X_train, y_train, epochs = 20, batch_size = 10, verbose = True, validation_data = (X_test, y_test))

loss, accuracy = modelo.evaluate(X_test, y_test)
print("Loss: ", loss)
print("Accuracy: ", accuracy)

nova_previsao = modelo.predict(X_test)
print(nova_previsao)

prev = (nova_previsao > 0.5)
print(prev)

cm = confusion_matrix(y_test, prev)
print(cm)