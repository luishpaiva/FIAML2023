# -*- coding: utf-8 -*-
"""nltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LcQvcGbNaYHhHwtyy1QqTdZgth9Le_-L
"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag, pos_tag_sents
import string

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("tagsets")
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")
nltk.download("maxent_ne_chunker")
nltk.download("words")

Texto = '''Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o
cosmos se autoconhecer. A imaginação nos leva a mundos que nunca
sequer existiram. Mas sem ela não vamos a lugar algum. '''

print(Texto)

sentencas = sent_tokenize(Texto, language = "portuguese")
print(type(sentencas))
print(sentencas)

print(len(sentencas))

tokens = word_tokenize(Texto, language = "portuguese")
print(tokens)

stops = stopwords.words("portuguese")
print(stops)

palavras_sem_stopwords = [p for p in tokens if p not in stops]
print(palavras_sem_stopwords)

print(len(tokens), "   ", len(palavras_sem_stopwords))

print(string.punctuation)

palavras_sem_pontuacao = [p for p in palavras_sem_stopwords if p not in string.punctuation]
print(palavras_sem_pontuacao)

frequencia = nltk.FreqDist(palavras_sem_pontuacao)
frequencia

mais_comuns = frequencia.most_common(3)
mais_comuns

stemmer = PorterStemmer()
stem1 = [stemmer.stem(word) for word in palavras_sem_pontuacao]
print(palavras_sem_pontuacao)
print(stem1)

stemmer2 = SnowballStemmer("portuguese")
stem2 = [stemmer2.stem(word) for word in palavras_sem_pontuacao]
print(palavras_sem_pontuacao)
print(stem2)

stemmer3 = LancasterStemmer()
stem3 = [stemmer3.stem(word) for word in palavras_sem_pontuacao]
print(palavras_sem_pontuacao)
print(stem3)

nltk.help.upenn_tagset()

pos = nltk.pos_tag(palavras_sem_pontuacao)
print(pos)

lemmatizer = WordNetLemmatizer()
resultado = [lemmatizer.lemmatize(palavra) for palavra in palavras_sem_stopwords]
print(palavras_sem_stopwords)
print(resultado)

texto_en = "Curitiba é a capital do estado do Paraná"
token3 = word_tokenize(texto_en)
tags = pos_tag(token3)
en = nltk.ne_chunk(tags)
print(en)

