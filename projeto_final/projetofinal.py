# -*- coding: utf-8 -*-
"""ProjetoFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMaLwy0ddcQmtB8Mr8EPGd_7_2gFrhxR

## 1. Importação das Bibliotecas
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout

from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix

"""## 2. Importação dos arquivos de treino, teste e validação"""

from google.colab import files
files.upload()

"""## 3. Pré-Processamento"""

# Carregando os dados dos arquivos em um dataframe
train = pd.read_csv("train.csv", sep = ",")
test = pd.read_csv("test.csv", sep = ",")

train.shape

test.shape

# Concatenando os conjuntos de treino e de teste para poder fazer o
# pré-processamento uma única vez
df = pd.concat([train, test])

"""### 3.1. Conhecendo o conjunto de dados"""

# Visualizando uma parte aletória do conjunto para identificar valores
# não adequados
df.sample(10)

df.shape

df.info()

# Removendo a variável 'education', pois já se tem os valores em formato
# numérico ('educational-num')
df.drop(columns = 'education', inplace = True)

df

"""### 3.2. Tratamento de valores ausentes (NaN)"""

# Verificando o total de valores nulos
df.isnull().sum()

# Como verificado no item 3, alguns valores estão demarcados como '?'
# Verificando o total de valores '?'
df.isin(['?']).sum()

# Verificando a moda em cada uma das categorias que possuem NaNs
df.groupby(['workclass']).size()

df.groupby(['occupation']).size()

df.groupby(['native-country']).size()

# Atualizando os '?' com NaNs
df = df.replace('?', np.NaN)

# Atualizando os NaNs com a moda
df['workclass'].fillna('Private', inplace = True)
df['occupation'].fillna('Prof-specialty', inplace = True)
df['native-country'].fillna('United-States', inplace = True)

# Apresentando valores atualizados
display(df.iloc[30:41])

"""### 3.3. Label Encoding"""

# Criando um vetor com o nome das variáveis categóricas
categoricals = [
    'workclass',
    'marital-status',
    'occupation',
    'relationship',
    'race',
    'gender',
    'native-country',
    'income'
]

categoricals

# Criando um vetor com as variáveis numéricas
numericals = list(df.select_dtypes(include=['int64', 'float64']).columns)

numericals

# Transformando os valores para 'float'
var_float = df.loc[:, numericals].var()
for i in range(len(numericals)):
    print('{} \t\t {}'.format(numericals[i], round(float(var_float[i]), 3)))

# Transformando as variáveis categóricas
label_encoder = LabelEncoder()
for i in categoricals:
  df[i] = pd.DataFrame(label_encoder.fit_transform(df[i]))

df.head()

"""### 3.4 Scaling"""

min_max_scaler = MinMaxScaler()

column_values = df.columns.values
column_values = column_values[:-1]

scaled_values = min_max_scaler.fit_transform(df[column_values])

for i in range(len(column_values)):
    df[column_values[i]] = scaled_values[:,i]

df.sample(10)

"""### 3.5. Outliers

"""

# Calculando o desvio padrão das variáveis numéricas
dp_age = df['age'].std()
dp_fnlwgt = df['fnlwgt'].std()
dp_capital_gain = df['capital-gain'].std()
dp_capital_loss = df['capital-loss'].std()
dp_hours_per_week = df['hours-per-week'].std()

print("Desvio Padrão 'age': ", dp_age)
print("Desvio Padrão 'fnlwgt': ", dp_fnlwgt)
print("Desvio Padrão 'capital-gain': ", dp_capital_gain)
print("Desvio Padrão 'capital-loss': ", dp_capital_loss)
print("Desvio Padrão 'hours-per-week': ", dp_hours_per_week)

"""#### 3.5.1. Visualizando os outliers"""

# Verificando se há outliers no conjunto
df.loc[df['age'] >= 3 * dp_age, 'age']

plt.figure(figsize=(8,5))
sns.displot(df['age'], kde = True)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x="age", data = df)
plt.show()

df.loc[df['fnlwgt'] >= 3 * dp_fnlwgt, 'fnlwgt']

plt.figure(figsize=(8,5))
sns.displot(df['fnlwgt'], kde = True)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x="fnlwgt", data = df)
plt.show()

df.loc[df['capital-gain'] >= 3 * dp_capital_gain, 'capital-gain']

plt.figure(figsize=(8,5))
sns.displot(df['capital-gain'], kde = True)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x="capital-gain", data = df)
plt.show()

df.loc[df['capital-loss'] >= 3 * dp_capital_loss, 'capital-loss']

plt.figure(figsize=(8,5))
sns.displot(df['capital-loss'], kde = True)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x="capital-loss", data = df)
plt.show()

df.loc[df['hours-per-week'] >= 3 * dp_hours_per_week, 'hours-per-week']

plt.figure(figsize=(8,5))
sns.displot(df['hours-per-week'], kde = True)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x="hours-per-week", data = df)
plt.show()

"""#### 3.5.2. Tratando os outliers"""

# Função para definir os limites (inferior e superior) de uma varável
def outlier_bounds(column):
  Q1, Q3 = np.percentile(column, [25, 75])
  IQR = Q3 - Q1
  lower_bound = Q1 - (1.5 * IQR)
  upper_bound = Q3 + (1.5 * IQR)
  return lower_bound, upper_bound

# Definindo limites inferior e superior para a variável 'age'
lower_bound, upper_bound = outlier_bounds(df['age'])

print("Limite inferior 'age': ", lower_bound)
print("Limite superior 'age': ", upper_bound)

# Definindo as linhas que possuem outliers para a variável 'age'
df[(df.age < lower_bound) | (df.age > upper_bound)]

# Atualizando o valor do vetor 'numericals', desconsiderando 'capital-gain' e
# 'capital-loss' do tratamento de outliers, pois possuem valores esparsos
numericals.remove('capital-gain')
numericals.remove('capital-loss')
numericals

# Tratamento dos outliers
for i in numericals:
  lower_bound, upper_bound = outlier_bounds(df[i])
  median = df[i].median()
  df.loc[(df[i] < lower_bound) | (df[i] > upper_bound), i] = median
  print('Variável \'{}\'\nOutliers: {}\n'.format(i, df[
    (df[i] < lower_bound) | (df[i] > upper_bound)][i]))

# Mapa de Calor de Correlação (Correlation Heatmap)
fig, ax = plt.subplots(figsize=(16,12))
ax = sns.heatmap(df.corr(), annot = True)

# Nenhuma correlação foi maior que 0.7 ou menor que -0.7;
# portanto, pode-se afirmar que não correlação entre as variáveis

# Criando os conjuntos de treino e de test
X = df.iloc[:,:-1] # ou X = all_columns[:-1]
y = df['income']

print(X)
print(y.shape)

"""### 4. Treinamento

### 4.1. Divisão do conjunto de dados em treino e teste
"""

# Dividindo o dataframe em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22)

# Quando os dados foram concatenados, treino tinha 34189 registros e teste tinha
# 7327 registros. Para manter a proporção, optou-se por dividir em 78% e 22%
# (arredondando-se)

# Verificando a divisão dos conjuntos de treino e teste
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""### 4.2. Modelos

#### 4.2.1. Logistic Regression (Regressão Logística)
"""

logistic_regressor = LogisticRegression()
logistic_regressor.fit(X_train, y_train)

logistic_train_score = logistic_regressor.score(X_train, y_train)
logistic_test_score = logistic_regressor.score(X_test, y_test)
logistic_prediction = logistic_regressor.predict(X_test)

print('Train Score: {0}\nTest Score: {1}'.format(logistic_train_score, logistic_test_score))

logistic_as = accuracy_score(y_test, logistic_prediction)
logistic_f1 = f1_score(y_test, logistic_prediction)
logistic_rs = recall_score(y_test, logistic_prediction)
logistic_cm = confusion_matrix(y_test, logistic_prediction)

print(f'Accuracy Score:\n{logistic_as}\n')
print(f'F1 Score:\n{logistic_f1}\n')
print(f'Recall Score:\n{logistic_rs}\n')
print(f'Confusion Matrix:\n{logistic_cm}\n')

"""#### 4.2.2 K-Nearest Neighbors Classifier (K-ésimo Vizinho mais Próximo)"""

error_rate = []
k_values = list(filter(lambda x: x%2 == 1, range(0, 50)))
best_k = 0

for i in k_values:
  knn = KNeighborsClassifier(n_neighbors = i)
  knn.fit(X_train, y_train)
  pred_i = knn.predict(X_test)
  error_rate.append(np.mean(pred_i != y_test))
print(error_rate.index(np.min(error_rate)))

plt.figure(figsize=(10,10))
plt.plot(k_values, error_rate,color = 'blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize = 10)
plt.title('Taxa de Erro x Valor de K')
plt.xlabel('K')
plt.ylabel('Taxa de Erro')

# 2 * index + 1, o index vale 15, como verificado acima
knn = KNeighborsClassifier(n_neighbors = 31)
modelo = knn.fit(X, y)

knn_prev = modelo.predict(X)
print(knn_prev)

accuracy = accuracy_score(y, knn_prev)
precision = precision_score(y, knn_prev, average = 'weighted')
recall = recall_score(y, knn_prev, average = 'weighted')
f1 = f1_score(y, knn_prev, average = 'weighted')
cm = confusion_matrix(y, knn_prev)

print(f'Accuracy Score:\n{accuracy}\n')
print(f'F1 Score:\n{f1}\n')
print(f'Recall Score:\n{recall}\n')
print(f'Confusion Matrix:\n{cm}\n')

"""#### 4.2.3 Support Vector Classifier (Máquina de Vetores de Suporte)"""

svc = SVC(kernel = 'rbf')
svc.fit(X_train, y_train)

y_prev = svc.predict(X_test)
print(y_prev)

accuracy = accuracy_score(y_test, y_prev)
precision = precision_score(y_test, y_prev, average = 'weighted')
recall = recall_score(y_test, y_prev, average = 'weighted')
f1 = f1_score(y_test, y_prev, average = 'weighted')
cm = confusion_matrix(y_test, y_prev)

print(f'Accuracy Score:\n{accuracy}\n')
print(f'F1 Score:\n{f1}\n')
print(f'Recall Score:\n{recall}\n')
print(f'Confusion Matrix:\n{cm}\n')

"""#### 4.2.4 Decision Tree (Árvore de Decisão)"""

modelo = DecisionTreeClassifier(random_state = 1, max_depth = 8, max_leaf_nodes = 2)
modelo.fit(X_train, y_train)

y_prev = modelo.predict(X_test)

accuracy = accuracy_score(y_test, y_prev)
precision = precision_score(y_test, y_prev, average = 'weighted', zero_division = 1)
recall = recall_score(y_test, y_prev, average = 'weighted')
f1 = f1_score(y_test, y_prev, average = 'weighted')
cm = confusion_matrix(y_test, y_prev)

print(f'Accuracy Score:\n{accuracy}\n')
print(f'F1 Score:\n{f1}\n')
print(f'Recall Score:\n{recall}\n')
print(f'Confusion Matrix:\n{cm}\n')

"""#### 4.2.5. Neural Networks (Redes Neurais)"""

model = Sequential()
model.add(Dense(units = 64, activation = 'relu', input_dim = X_train.shape[1]))
model.add(Dropout(0.4))
model.add(Dense(units = 32, activation = 'relu'))
model.add(Dropout(0.4))
model.add(Dense(units = 64, activation = 'relu'))
model.add(Dropout(0.4))
model.add(Dense(units = 1, activation = 'sigmoid'))

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, epochs = 100, batch_size = 32)

previsoes = model.predict(X_test)
previsoes

y_prev = (previsoes > 0.5).astype('int32')
y_prev

accuracy = accuracy_score(y_test, y_prev)
precision = precision_score(y_test, y_prev, average = 'weighted', zero_division = 1)
recall = recall_score(y_test, y_prev, average = 'weighted')
f1 = f1_score(y_test, y_prev, average = 'weighted')
cm = confusion_matrix(y_test, y_prev)

print(f'Accuracy Score:\n{accuracy}\n')
print(f'F1 Score:\n{f1}\n')
print(f'Recall Score:\n{recall}\n')
print(f'Confusion Matrix:\n{cm}\n')